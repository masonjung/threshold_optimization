{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5d0d33b-20a3-4042-86bf-c77401f65fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b32524a3-e9f6-45ee-9927-537ab4bdd3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/msjung/.cache/huggingface/datasets/yaful___csv/yaful--MAGE-0712bb75356d570f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label        src\n",
      "0  White girls very rarely date Asian men. Even i...      1  cmv_human\n",
      "1  I am a 23 year old male Indian American male. ...      1  cmv_human\n",
      "2  Take three people, Persons A, B, and C. They l...      1  cmv_human\n",
      "3  (A) Work part-time in high school; Then go to ...      1  cmv_human\n",
      "4  When police introduce a new form of speed prev...      1  cmv_human\n"
     ]
    }
   ],
   "source": [
    "# Load the MAGE dataset from Hugging Face\n",
    "dataset = load_dataset(\"yaful/MAGE\", split = \"train\")\n",
    "\n",
    "# Convert the 'test' split to a Pandas DataFrame\n",
    "df_test = pd.DataFrame(dataset)\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e5d4c1c-de2d-4cd1-a545-ad78b4a4bde5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  label  \\\n",
      "134255  Jim is shocked when he gets an amazing deal on...      0   \n",
      "130746  I don't want to go to the fair. It's too hot a...      0   \n",
      "260567  I've been to the darkest places, so dark, so e...      1   \n",
      "243344  It's impossible to make a plane that flies the...      1   \n",
      "227956  Understanding the formation of subjective huma...      0   \n",
      "\n",
      "                                          src  \n",
      "134255     roct_machine_continuation_opt_6.7b  \n",
      "130746  roct_machine_topical_text-davinci-002  \n",
      "260567                               wp_human  \n",
      "243344                             eli5_human  \n",
      "227956    sci_gen_machine_continuation_t0_11b  \n"
     ]
    }
   ],
   "source": [
    "# Take 1% of the dataset using the sample() method\n",
    "df_sample = df_test.sample(n=1000)\n",
    "\n",
    "# Display the first few rows of the sampled dataset\n",
    "print(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db7c22b6-96dc-41b3-bd93-d72078e4852b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    725\n",
       "1    275\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a20c6363-8098-4592-8d90-82ac386648a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  label  \\\n",
      "134255  Jim is shocked when he gets an amazing deal on...      0   \n",
      "130746  I don't want to go to the fair. It's too hot a...      0   \n",
      "260567  I've been to the darkest places, so dark, so e...      1   \n",
      "243344  It's impossible to make a plane that flies the...      1   \n",
      "227956  Understanding the formation of subjective huma...      0   \n",
      "\n",
      "                                          src  char_count  \n",
      "134255     roct_machine_continuation_opt_6.7b         878  \n",
      "130746  roct_machine_topical_text-davinci-002         770  \n",
      "260567                               wp_human        2785  \n",
      "243344                             eli5_human        1952  \n",
      "227956    sci_gen_machine_continuation_t0_11b         230  \n"
     ]
    }
   ],
   "source": [
    "# Add a new column that counts the number of characters in the 'text' column\n",
    "df_sample['char_count'] = df_sample['text'].str.len()\n",
    "\n",
    "# Display the first few rows of the DataFrame with the new 'char_count' column\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2839f297-a1a7-4100-90c3-d3d2001c326e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean     1213.508000\n",
       "std      1353.142313\n",
       "min        33.000000\n",
       "25%       282.000000\n",
       "50%       649.000000\n",
       "75%      1564.750000\n",
       "max      7933.000000\n",
       "Name: char_count, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['char_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7028de-2058-4013-aeca-1a2cc2b845cc",
   "metadata": {},
   "source": [
    "# AI Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b5f78a-955c-4a53-ad81-e86420bde50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f0b4c48-9f0d-4f73-b222-cfe3d83529c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3757344-a0aa-41ce-9119-bc373bbf92fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/msjung/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bf33f56-f22c-4feb-a25c-dbb8410f4f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = 'openai-community/roberta-base-openai-detector'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model2 = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model2.eval()\n",
    "\n",
    "def classify_text2(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "    # probabilities[0] corresponds to \"human-written\"\n",
    "    # probabilities[1] corresponds to \"AI-generated\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dddb0ef4-3215-4bdb-992d-b7a0fcf5c041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clasisification\n",
    "def roberta_base_openai_detector_probability(text):\n",
    "    probabilities = classify_text2(text)\n",
    "    ai_probability = probabilities[1]  # Probability of being AI-generated\n",
    "    return ai_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d43ca5a-8433-4433-9a46-c45de1ade2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample['roberta_base_openai_detector_probability'] = df_sample['text'].apply(roberta_base_openai_detector_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0b6a7-29da-4bd9-9e6b-9362802d138e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the updated DataFrame with AI-generated probabilities\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e9f54-30c8-4e85-af93-e434d3f87900",
   "metadata": {},
   "source": [
    "# OpenAI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094384c-3ec5-40b9-9f3a-5434da333024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = 'openai-community/roberta-large-openai-detector'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model3 = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model3.eval()\n",
    "\n",
    "def classify_text3(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model3(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "    # probabilities[0] corresponds to \"human-written\"\n",
    "    # probabilities[1] corresponds to \"AI-generated\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14ca64-15d2-4564-adb0-fb6f79a11e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clasisification\n",
    "def roberta_large_openai_detector_probability(text):\n",
    "    probabilities = classify_text3(text)\n",
    "    ai_probability = probabilities[1]  # Probability of being AI-generated\n",
    "    return ai_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cc3a1-6c31-43cd-9478-8633a22a2e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sample['roberta_large_openai_detector_probability'] = df_sample['text'].apply(roberta_large_openai_detector_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2f3c7-1a1c-4070-adba-4ecb58e4d076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the updated DataFrame with AI-generated probabilities\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac1d6b-51bc-41f5-8891-bc200176c40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sample.to_csv('mage_test_d2.csv', index=False, encoding='utf-8')\n",
    "print(\"CSV file 'mage_test_d2.csv' has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc1a47-0a77-4ecf-88e6-0252b7460986",
   "metadata": {},
   "source": [
    "# mage_d2 datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e278c1-57d2-43b0-9e3a-c977858fbca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mage_test_d2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b90855-5216-4f52-aa6d-5b9c06894c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_correlations(df, x_column, y_columns, figsize=(14, 6), alpha=0.5):\n",
    "    correlations = {}\n",
    "    num_plots = len(y_columns)\n",
    "    \n",
    "    # Initialize the plotting area\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i, y_col in enumerate(y_columns, 1):\n",
    "        # Calculate Pearson correlation coefficient\n",
    "        corr = df[x_column].corr(df[y_col])\n",
    "        correlations[y_col] = corr\n",
    "        \n",
    "        # Create a subplot for each y_column\n",
    "        plt.subplot(1, num_plots, i)\n",
    "        plt.scatter(df[x_column], df[y_col], alpha=alpha)\n",
    "        plt.title(f'{x_column} vs {y_col}\\nCorrelation: {corr:.3f}')\n",
    "        plt.xlabel(x_column)\n",
    "        plt.ylabel(y_col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63ac13-0e22-423d-a0df-daba9c53587f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df\n",
    "\n",
    "# Define the x-axis and y-axis columns\n",
    "x_col = 'char_count'\n",
    "y_cols = [\n",
    "    'roberta_base_openai_detector_probability',\n",
    "    'roberta_large_openai_detector_probability'\n",
    "]\n",
    "\n",
    "# Call the function and store the correlations\n",
    "correlation_results = plot_correlations(data, x_col, y_cols)\n",
    "\n",
    "# Print the correlation results\n",
    "for y_col, corr in correlation_results.items():\n",
    "    print(f'Correlation between {x_col} and {y_col}: {corr:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb5c2c-b6fc-492c-b2aa-98fe2c55b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_char_count_distribution(df, threshold=100):\n",
    "    \"\"\"\n",
    "    Plots the distribution of character counts and marks the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the data.\n",
    "    - threshold (int, optional): The character count threshold. Defaults to 100.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['char_count'], bins=100, kde=True, color='skyblue')\n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')\n",
    "    plt.title('Character Count Distribution')\n",
    "    plt.xlabel('Character Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a80b8-dbfc-42ca-ad45-fc0a7705e277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "plot_char_count_distribution(df, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9f163-a9c0-438f-8bfe-c8972d496c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_short_wrt = df[df[\"char_count\"] < 30**2]\n",
    "df_long_wrt = df[df[\"char_count\"] >= 30**2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af76ef5-dfb6-409b-8d1d-ab77fc1d5cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_length_category(df, char_count_column='char_count', threshold=1000):\n",
    "    df['long_wrt'] = df[char_count_column].apply(lambda x: 0 if x < threshold else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3ab00-d51f-49cb-a1a2-46aef99e61bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_length_category(df, char_count_column='char_count', threshold=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351496a-96b4-48ec-84ee-c3e8f7aa3bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the list of probability columns to plot\n",
    "probability_columns = [\n",
    "    # 'DetectGPT_probability',\n",
    "    'roberta_base_openai_detector_probability',\n",
    "    'roberta_large_openai_detector_probability'\n",
    "]\n",
    "\n",
    "# Combine both datasets\n",
    "combined_df = pd.concat([df[df['long_wrt'] == 1].assign(Group='Long writing'),\n",
    "                         df[df['long_wrt'] == 0].assign(Group='Short writing')])\n",
    "\n",
    "# Loop through each probability column and plot a boxplot\n",
    "for column in probability_columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    combined_df.boxplot(column=column, by='Group')\n",
    "    plt.title(f'Long v. Short Writing Box Plot for {column}')\n",
    "    plt.suptitle('')  # Remove default suptitle\n",
    "    plt.ylabel('Probability of AI written')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248af6c-81d4-4b08-9dde-c6169531a140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probability_columns = [\n",
    "    # 'DetectGPT_probability',\n",
    "    'roberta_base_openai_detector_probability',\n",
    "    'roberta_large_openai_detector_probability'\n",
    "]\n",
    "\n",
    "# threshold of 0.5\n",
    "def apply_static_threshold(df, threshold=0.5):\n",
    "    for column in probability_columns:\n",
    "        new_col = f'{column}_static_threshold'\n",
    "        df[new_col] = df[column].apply(lambda prob: 1 if prob >= threshold else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb10327-815c-401e-902e-1f67d138d950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = apply_static_threshold(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dac195-4f54-491c-a3dc-9930cb70de91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the probability columns for which confusion matrices will be generated\n",
    "probability_columns = [\n",
    "    # 'DetectGPT_probability_static_threshold',\n",
    "    'roberta_base_openai_detector_probability_static_threshold',\n",
    "    'roberta_large_openai_detector_probability_static_threshold'\n",
    "]\n",
    "\n",
    "# Filter the dataset for long and short writings based on 'long_wrt' column\n",
    "df_long_wrt = df[df['long_wrt'] == 1]\n",
    "df_short_wrt = df[df['long_wrt'] == 0]\n",
    "\n",
    "# Function to calculate and display the confusion matrix\n",
    "def calculate_confusion_matrix(df, y_true_col, y_pred_col, group_name):\n",
    "    y_true = df[y_true_col]\n",
    "    y_pred = df[y_pred_col]\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Print confusion matrix and other relevant metrics\n",
    "    print(f\"Confusion Matrix for {group_name} - {y_pred_col}:\\n{cm}\")\n",
    "    print(f\"True Negatives: {tn}, False Positives: {fp}, False Negatives: {fn}, True Positives: {tp}\\n\")\n",
    "\n",
    "# Iterate through the probability columns to calculate confusion matrices for both long and short writing\n",
    "for prob_col in probability_columns:\n",
    "    # Confusion matrix for long writings\n",
    "    calculate_confusion_matrix(df_long_wrt, 'label', prob_col, 'Long Writing')\n",
    "\n",
    "    # Confusion matrix for short writings\n",
    "    calculate_confusion_matrix(df_short_wrt, 'label', prob_col, 'Short Writing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b2164-316f-4bba-b526-b3b8d9f096de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate FPR\n",
    "def calculate_fpr(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # Prevent division by zero\n",
    "    return fpr\n",
    "\n",
    "# Define the probability columns for which FPR will be calculated\n",
    "probability_columns = [\n",
    "    # 'DetectGPT_probability_static_threshold',\n",
    "    'roberta_base_openai_detector_probability_static_threshold',\n",
    "    'roberta_large_openai_detector_probability_static_threshold'\n",
    "]\n",
    "\n",
    "# Split data into long writing and short writing based on 'long_wrt' column\n",
    "df_long_wrt = df[df['long_wrt'] == 1]\n",
    "df_short_wrt = df[df['long_wrt'] == 0]\n",
    "\n",
    "# Iterate over the probability columns and calculate FPR for both long and short writings\n",
    "fpr_results = {}\n",
    "for prob_col in probability_columns:\n",
    "    # Calculate FPR for long writing group\n",
    "    y_true_long = df_long_wrt['label']  # Ground truth for long writing\n",
    "    y_pred_long = df_long_wrt[prob_col]  # Predicted values for long writing\n",
    "    fpr_long = calculate_fpr(y_true_long, y_pred_long)\n",
    "\n",
    "    # Calculate FPR for short writing group\n",
    "    y_true_short = df_short_wrt['label']  # Ground truth for short writing\n",
    "    y_pred_short = df_short_wrt[prob_col]  # Predicted values for short writing\n",
    "    fpr_short = calculate_fpr(y_true_short, y_pred_short)\n",
    "\n",
    "    # Store the results in a dictionary for easy access\n",
    "    fpr_results[prob_col] = {'Long Writing': fpr_long, 'Short Writing': fpr_short}\n",
    "\n",
    "# Now, plot FPR for each probability column\n",
    "for prob_col, fpr_data in fpr_results.items():\n",
    "    # Create the data for the plot\n",
    "    groups = ['Long Writing', 'Short Writing']\n",
    "    fpr_values = [fpr_data['Long Writing'], fpr_data['Short Writing']]\n",
    "\n",
    "    # Plot the FPR for each group\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(groups, fpr_values, color=['g', 'b'])\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.ylabel('Misclassification Rate (FPR)')\n",
    "    plt.title(f'Misclassification Rate (FPR) for {prob_col}')\n",
    "\n",
    "    # Annotate the bars with the FPR values (percentages)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.2%}', ha='center', va='bottom')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4012bdd-81e7-43cf-833a-dfa606024f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)  # Calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Define the probability columns for which accuracy will be calculated\n",
    "probability_columns = [\n",
    "    # 'DetectGPT_probability_static_threshold',\n",
    "    'roberta_base_openai_detector_probability_static_threshold',\n",
    "    'roberta_large_openai_detector_probability_static_threshold'\n",
    "]\n",
    "\n",
    "# Split data into long writing and short writing based on 'long_wrt' column\n",
    "df_long_wrt = df[df['long_wrt'] == 1]\n",
    "df_short_wrt = df[df['long_wrt'] == 0]\n",
    "\n",
    "# Iterate over the probability columns and calculate accuracy for both long and short writings\n",
    "accuracy_results = {}\n",
    "for prob_col in probability_columns:\n",
    "    # Calculate accuracy for long writing group\n",
    "    y_true_long = df_long_wrt['label']  # Ground truth for long writing\n",
    "    y_pred_long = df_long_wrt[prob_col]  # Predicted values for long writing\n",
    "    accuracy_long = calculate_accuracy(y_true_long, y_pred_long)\n",
    "\n",
    "    # Calculate accuracy for short writing group\n",
    "    y_true_short = df_short_wrt['label']  # Ground truth for short writing\n",
    "    y_pred_short = df_short_wrt[prob_col]  # Predicted values for short writing\n",
    "    accuracy_short = calculate_accuracy(y_true_short, y_pred_short)\n",
    "\n",
    "    # Store the results in a dictionary for easy access\n",
    "    accuracy_results[prob_col] = {'Long Writing': accuracy_long, 'Short Writing': accuracy_short}\n",
    "\n",
    "# Now, plot accuracy for each probability column\n",
    "for prob_col, accuracy_data in accuracy_results.items():\n",
    "    # Create the data for the plot\n",
    "    groups = ['Long Writing', 'Short Writing']\n",
    "    accuracy_values = [accuracy_data['Long Writing'], accuracy_data['Short Writing']]\n",
    "\n",
    "    # Plot the accuracy for each group\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(groups, accuracy_values, color=['g', 'b'])\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy for {prob_col}')\n",
    "\n",
    "    # Annotate the bars with the accuracy values (percentages)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.2%}', ha='center', va='bottom')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680bcf0-26ce-46af-b1bf-f9e97f5a890e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate F1 score\n",
    "def calculate_f1_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1_score\n",
    "\n",
    "# Define the probability columns for which F1 score will be calculated\n",
    "probability_columns = [\n",
    "    # 'DetectGPT_probability_static_threshold',\n",
    "    'roberta_base_openai_detector_probability_static_threshold',\n",
    "    'roberta_large_openai_detector_probability_static_threshold'\n",
    "]\n",
    "\n",
    "\n",
    "# Iterate over the probability columns and calculate F1 score for both long and short writings\n",
    "f1_score_results = {}\n",
    "for prob_col in probability_columns:\n",
    "    # Calculate F1 score for long writing group\n",
    "    y_true_long = df_long_wrt['label']  # Ground truth for long writing\n",
    "    y_pred_long = df_long_wrt[prob_col]  # Predicted values for long writing\n",
    "    f1_score_long = calculate_f1_score(y_true_long, y_pred_long)\n",
    "\n",
    "    # Calculate F1 score for short writing group\n",
    "    y_true_short = df_short_wrt['label']  # Ground truth for short writing\n",
    "    y_pred_short = df_short_wrt[prob_col]  # Predicted values for short writing\n",
    "    f1_score_short = calculate_f1_score(y_true_short, y_pred_short)\n",
    "\n",
    "    # Store the results in a dictionary for easy access\n",
    "    f1_score_results[prob_col] = {'Long Writing': f1_score_long, 'Short Writing': f1_score_short}\n",
    "\n",
    "# Now, plot F1 score for each probability column\n",
    "for prob_col, f1_score_data in f1_score_results.items():\n",
    "    # Create the data for the plot\n",
    "    groups = ['Long Writing', 'Short Writing']\n",
    "    f1_score_values = [f1_score_data['Long Writing'], f1_score_data['Short Writing']]\n",
    "\n",
    "    # Plot the F1 score for each group\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(groups, f1_score_values, color=['g', 'b'])\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'F1 Score for {prob_col}')\n",
    "\n",
    "    # Annotate the bars with the F1 score values (percentages)\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.2%}', ha='center', va='bottom')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc441fa-823b-41fc-a093-dadf400a1c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpreting equaltion\n",
    "def equation_fairness(Prob_a,Prob_b):\n",
    "    rule = 0.8 # threshold of 0.8 is based on 80% rule.\n",
    "    greater = max(Prob_a, Prob_b)\n",
    "    smaller = min(Prob_a, Prob_b)\n",
    "    evaluation = (smaller/greater) > rule # if the ratio exceeds, it is considered fair = True\n",
    "    return evaluation\n",
    "    # Based on Feldman, M., etc. (2015) Certifying and removing disparate impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd225f1f-8413-4f3d-9b23-e64869d50f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daeaba-24e3-4087-bf5b-5efe72a5f30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8077922-dc1e-428d-bbf2-f8fd4a5479b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da0e8e-eb6a-4eae-85b3-f0de59b393ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da36ab1-a263-4baa-817e-fa687c2caee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
