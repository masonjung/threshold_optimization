# Libraries
import transformers
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

# Initialize Llama Model
model_id = "meta-llama/Llama-3.1-8B"
# model_id = "meta-llama/Meta-Llama-3-70B"
# model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)

# Check for CUDA availability
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
    use_auth_token=True
)


# Dataset
df = pd.read_csv("C:\\Users\\minse\\Desktop\\Programming\\FairThresholdOptimization\\datasets\\train_features.csv")

# Sample a small fraction of the dataset
df = df.sample(frac=0.0005)
print(df.shape)


def llama_ai_probability(text):
    # Refine the prompt to be clear and explicit, with minimal ambiguity
    prompt = (
        "You are an AI text detector that estimates the probability that a given text was written by an AI. Respond only with a single numerical probability value between 0 and 1, formatted to 7 decimal points."
        f"Estimate the probability that the following text was generated by an AI:\n\n{text}\n\n"
        #  "The output must be formatted as a single number between 0.0000000 and 1.0000000, nothing else."
    )

    try:
        # Tokenize the prompt
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        # Generate a response from the model
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,  # Enough tokens to get a single number
                temperature=0.0,    # Deterministic output
                do_sample=False     # Ensure consistent response
            )

        # Decode the output tensor to get the generated text
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # Print the response to see what the model returned
        print("Model response:", response)

        # Use regex to extract a probability in the response
        match = re.search(r"\b0\.\d{1,7}\b|\b1\.0000000\b", response)
        if match:
            probability = float(match.group(0))
            return probability
        else:
            # Retry with a modified prompt or handle as no valid response
            print("No valid probability found in response:", response)
            return None

    except Exception as e:
        print(f"Error: {e}")
        return None
    
# Assuming `df` and `df['essay']` exist
df['Llama3.1-8B_probability'] = df['essay'].apply(lambda x: llama_ai_probability(x))
# Print the first few rows to verify results
print(df[['essay', 'Llama3.1-8B_probability']])
