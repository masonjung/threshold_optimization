# Libraries
import transformers
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

# Initialize Llama Model
model_id = "meta-llama/Llama-3.1-8B"
# model_id = "meta-llama/Meta-Llama-3-70B"
# model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)

# Check for CUDA availability
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
    use_auth_token=True
)


# Dataset
df = pd.read_csv("C:\\Users\\minse\\Desktop\\Programming\\FairThresholdOptimization\\datasets\\train_features.csv")

# Sample a small fraction of the dataset
df = df.sample(frac=0.0005)
print(df.shape)



def llama_ai_probability(text):
    # Improved prompt: more explicit instructions at the end
    prompt = (
        "You are an AI text detector that estimates the probability that a given text was written by an AI. "
        "You must respond ONLY with a single numerical probability value between 0 and 1 as a floating-point number with exactly 7 decimal places"
        "Do not add additional text or explanation.\n\n"
        "Text:\n" + text + "\n\n"
    )

    try:
        # Tokenize the prompt
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        # Generate a response from the model
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,  # allow a few tokens for the number
                temperature=0.0,    
                do_sample=False,
            )

        # Decode the output tensor to get the generated text
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only what the model generated (after the prompt).
        # Since the prompt is known, we can slice the response after the prompt length.
        generated_part = full_response[len(prompt):].strip()

        print("Model response:", generated_part)  # Debug: see what is generated

        # Relax the regex to find a floating-point number and then we can format it
        # Example: matches something like 0.1234567 or 1.0000000
        match = re.search(r"\b(0\.\d{7}|1\.0000000)\b", generated_part)
        
        if match:
            probability = float(match.group(0))
            return probability
        else:
            # If strict match fails, try a more lenient match and then clamp and format
            match = re.search(r"\b\d?\.\d+\b", generated_part)
            if match:
                raw_num = float(match.group(0))
                # Clamp between 0 and 1
                clamped_num = min(max(raw_num, 0.0), 1.0)
                # Format to 7 decimal places
                return float(f"{clamped_num:.7f}")
            else:
                print("No valid probability found in response:", generated_part)
                return None

    except Exception as e:
        print(f"Error: {e}")
        return None



# Assuming `df` and `df['essay']` exist
df['Llama3.1-8B_probability'] = df['essay'].apply(lambda x: llama_ai_probability(x))
# Print the first few rows to verify results
print(df[['essay', 'Llama3.1-8B_probability']])














# def llama_ai_probability(text):
#     # Refine the prompt to be clear and explicit, with minimal ambiguity
#     prompt = (
#         "You are an AI text detector that estimates the probability that a given text was written by an AI. Respond only with a single numerical probability value between 0 and 1, formatted to 7 decimal points."
#         f"Estimate the probability that the following text was generated by an AI:\n\n{text}\n\n"
#         #  "The output must be formatted as a single number between 0.0000000 and 1.0000000, nothing else."
#     )

#     try:
#         # Tokenize the prompt
#         inputs = tokenizer(prompt, return_tensors="pt").to(device)

#         # Generate a response from the model
#         with torch.no_grad():
#             outputs = model.generate(
#                 **inputs,
#                 max_new_tokens=10,  # Enough tokens to get a single number
#                 temperature=0.0,    # Deterministic output
#                 do_sample=False     # Ensure consistent response
#             )

#         # Decode the output tensor to get the generated text
#         response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

#         # Print the response to see what the model returned
#         print("Model response:", response)

#         # Use regex to extract a probability in the response
#         match = re.search(r"\b0\.\d{1,7}\b|\b1\.0000000\b", response)
#         if match:
#             probability = float(match.group(0))
#             return probability
#         else:
#             # Retry with a modified prompt or handle as no valid response
#             print("No valid probability found in response:", response)
#             return None

#     except Exception as e:
#         print(f"Error: {e}")
#         return None
    
# # Assuming `df` and `df['essay']` exist
# df['Llama3.1-8B_probability'] = df['essay'].apply(lambda x: llama_ai_probability(x))
# # Print the first few rows to verify results
# print(df[['essay', 'Llama3.1-8B_probability']])
